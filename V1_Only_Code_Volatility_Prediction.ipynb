{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a47377c",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Prework](#Prework)\n",
    "    - [Comparison between IV and HV](#Comparison_between_IV_and_HV)\n",
    "    - [Visualisaton for IV and HV](#Visualisaton_for_IV_and_HV)\n",
    "        - [Summary statistics](#Summary_statistics)\n",
    "        - [Data visualization](#Data_visualization)\n",
    "        - [Stationarity check](#Stationarity_check)\n",
    "        - [Performance metrics](#Performance_metrics)\n",
    "\n",
    "- [Build Model](#Build_Model)\n",
    "    - [Split trainning and testing dataset using VIX](#Split_trainning_and_testing_dataset_using_VIX)\n",
    "    - [Benchmark - Multi-Layer Perceptron (MLP)](#Benchmark_Multi-Layer_Perceptron_(MLP))\n",
    "    - [Build the ARIMA model using IV](#Build_the_ARIMA_model_using_IV)\n",
    "    - [Build the GARCH model using HV](#Build_the_GARCH_model_using_HV)\n",
    "    - [Build ANN model - LSTM](#Build_ANN_model_LSTM)\n",
    "    - [Build ANN model - GRU](#Build_ANN_model_GRU)\n",
    "    - [Build LSTM-ARIMA model](#Build_LSTM-ARIMA_model)\n",
    "    - [Build ARIMA-LSTM model](#Build_ARIMA-LSTM_model)\n",
    "    - [Build GRU-ARIMA model](#Build_GRU-ARIMA_model)\n",
    "    - [Build ARIMA-GRU model](#Build_ARIMA-GRU_model)\n",
    "\n",
    "- [Strategies](#Strategies)\n",
    "    - [Combine models prediction results](#Combine_models_prediction_results)\n",
    "    - [Evaluation method 1: index metrics](#Evaluation_method_1_index_metrics)\n",
    "    - [Evaluation method 2: trading strategies](#Evaluation_method_2_trading_strategies)\n",
    "    - [Evaluation method 3: Option pricing](#Evaluation_method_3_Option_pricing)\n",
    "        - [Call option](#Call_option)\n",
    "        - [Put option](#Put_option)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a26009",
   "metadata": {},
   "source": [
    "## Prework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373acdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.dates as mdates\n",
    "from scipy.optimize import minimize\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.stats.descriptivestats import describe\n",
    "from statsmodels.tsa.stattools import adfuller as adf\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import os\n",
    "from scipy.stats import norm\n",
    "import statsmodels.api as sm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import arch\n",
    "import scipy.stats as stats\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from arch import arch_model\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from pmdarima.arima import auto_arima\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a6781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data from downloaded excel files\n",
    "VIX = pd.read_excel(\"VIX.xlsx\",index_col = \"Date\")\n",
    "SP500 = pd.read_excel(\"S&P500.xlsx\",index_col = \"Date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e527e5fb",
   "metadata": {},
   "source": [
    "### Comparison_between_IV_and_HV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3dbb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_index = SP500.index.intersection(VIX.index)\n",
    "vix_sub_df = VIX.loc[common_index]\n",
    "# Calculate the historical volatility from the SP500 dataframe\n",
    "window_size = 30  # Adjust the window size as desired\n",
    "sp500_returns = SP500['Return'] # Calculate the daily returns\n",
    "historical_volatility = sp500_returns.rolling(window_size).std()  # Calculate rolling standard deviation\n",
    "\n",
    "# Normalize the data\n",
    "historical_volatility_normalized = (historical_volatility - historical_volatility.mean()) / historical_volatility.std()\n",
    "vix_normalized = (vix_sub_df['CBOE S&P500 Volatility Index - Close'] - vix_sub_df['CBOE S&P500 Volatility Index - Close'].mean()) / vix_sub_df['CBOE S&P500 Volatility Index - Close'].std()\n",
    "\n",
    "# Plot the normalized historical volatility and VIX data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(historical_volatility_normalized, label='Historical Volatility')\n",
    "plt.plot(vix_normalized, label='VIX')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Normalized Volatility')\n",
    "plt.title('Normalized Historical Volatility vs VIX')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8e3a67",
   "metadata": {},
   "source": [
    "### Visualisaton_for_IV_and_HV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c921f3",
   "metadata": {},
   "source": [
    "#### Summary_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3432be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(VIX['CBOE S&P500 Volatility Index - Close'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b041f7",
   "metadata": {},
   "source": [
    "#### Data_visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226bac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "# Create a line plot of the VIX close price\n",
    "plt.plot(VIX.index, VIX['CBOE S&P500 Volatility Index - Close'])\n",
    "\n",
    "# Set the title and axis labels\n",
    "plt.title('CBOE S&P500 Volatility Index')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close value')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67585d1b",
   "metadata": {},
   "source": [
    "#### Stationarity_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5985ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIX_close = VIX['CBOE S&P500 Volatility Index - Close'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f935e49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "\n",
    "# Plot rolling mean and standard deviation\n",
    "def plot_rolling_statistics(timeseries, window=7):\n",
    "    rolmean = timeseries.rolling(window=window).mean()\n",
    "    rolstd = timeseries.rolling(window=window).std()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(timeseries, label='Original')\n",
    "    plt.plot(rolmean, label='Rolling Mean')\n",
    "    plt.plot(rolstd, label='Rolling Std')\n",
    "    plt.legend()\n",
    "    plt.title('Rolling Mean and Standard Deviation')\n",
    "    plt.show()\n",
    "\n",
    "# Augmented Dickey-Fuller test\n",
    "def adf_test(timeseries):\n",
    "    result = adfuller(timeseries, autolag='AIC')\n",
    "    print(\"ADF Test Results:\")\n",
    "    print(f'ADF Statistic: {result[0]}')\n",
    "    print(f'p-value: {result[1]}')\n",
    "    print(f'Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print(f'   {key}: {value}')\n",
    "\n",
    "# Kwiatkowski-Phillips-Schmidt-Shin test\n",
    "def kpss_test(timeseries):\n",
    "    result = kpss(timeseries, regression='c')\n",
    "    print(\"KPSS Test Results:\")\n",
    "    print(f'KPSS Statistic: {result[0]}')\n",
    "    print(f'p-value: {result[1]}')\n",
    "    print(f'Critical Values:')\n",
    "    for key, value in result[3].items():\n",
    "        print(f'   {key}: {value}')\n",
    "\n",
    "# Check stationarity by plotting rolling statistics and conducting tests\n",
    "plot_rolling_statistics(VIX['CBOE S&P500 Volatility Index - Close'])\n",
    "\n",
    "adf_test(VIX_close)\n",
    "\n",
    "kpss_test(VIX_close)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c65287",
   "metadata": {},
   "source": [
    "#### Performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93785d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics\n",
    "def evaluation(test_data, model_predictions):\n",
    "    mse = mean_squared_error(test_data, model_predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(test_data, model_predictions)\n",
    "    r2 = r2_score(test_data, model_predictions)\n",
    "\n",
    "    print(\"Performance Metrics:\")\n",
    "    print(\"Mean Squared Error (MSE):\", mse)\n",
    "    print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    print(\"R-squared (R^2) Score:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f364dd",
   "metadata": {},
   "source": [
    "## Build_Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de01cb67",
   "metadata": {},
   "source": [
    "### Split_trainning_and_testing_dataset_using_VIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37018128",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert to the weekly dataset \n",
    "volatility_data = VIX['CBOE S&P500 Volatility Index - Close'].dropna().resample(\"W\").last()\n",
    "\n",
    "# Split the data into training, validation and test sets\n",
    "train_size = int(len(volatility_data)*0.8)\n",
    "test_size = len(volatility_data) - train_size\n",
    "\n",
    "train_data = volatility_data[:train_size]\n",
    "test_data = volatility_data[train_size:]\n",
    "train_dates = train_data.index\n",
    "test_dates = test_data.index\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(train_data, label=\"Training data\", color=\"blue\")\n",
    "plt.plot(test_data, label=\"Testing data\", color=\"orange\")\n",
    "plt.title(\"CBOE S&P500 Volatility Index data split\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Volatility\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8691770e",
   "metadata": {},
   "source": [
    "### Benchmark_Multi-Layer_Perceptron_(MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aff24e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "train_data_normalized = scaler.fit_transform(train_data.values.reshape(-1, 1))\n",
    "\n",
    "# Prepare the training data\n",
    "lookback = 60  # Number of previous time steps to consider\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(lookback, len(train_data_normalized)):\n",
    "    X_train.append(train_data_normalized[i - lookback:i, 0])\n",
    "    y_train.append(train_data_normalized[i, 0])\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "# Reshape the input data for LSTM (batch_size, timesteps, features)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1]))\n",
    "\n",
    "# Build the MLP model\n",
    "model_mlp = Sequential()\n",
    "model_mlp.add(Dense(units=64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model_mlp.add(Dense(units=32, activation='relu'))\n",
    "model_mlp.add(Dense(units=1))\n",
    "model_mlp.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the MLP model\n",
    "model_mlp.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Prepare the test data\n",
    "test_data_normalized = scaler.transform(test_data.values.reshape(-1, 1))\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for i in range(lookback, len(test_data_normalized)):\n",
    "    X_test.append(test_data_normalized[i - lookback:i, 0])\n",
    "    y_test.append(test_data_normalized[i, 0])\n",
    "\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "# Reshape the input data for MLP (batch_size, timesteps, features)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1]))\n",
    "\n",
    "# Make predictions on the test data\n",
    "predicted_values_mlp = model_mlp.predict(X_test)\n",
    "predicted_values_mlp = scaler.inverse_transform(predicted_values_mlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0fedae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_dates[lookback:], test_data[lookback:], color='green', label='Test Data')\n",
    "plt.plot(test_dates[lookback:], predicted_values_mlp, color='red', linestyle='dashed', label='Benchmark')\n",
    "plt.title('Test Data and Predicted Values (MLP)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('VIX index')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e077cf",
   "metadata": {},
   "source": [
    "### Build_the_ARIMA_model_using_IV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2836d3",
   "metadata": {},
   "source": [
    "Build a GARCH(1,1) model, and use the model to calculate fitted and predicted volatility of SP500 index. Plot the graph for original historical volatility and the predicted value, for the predicted value, use different color to show the fitted and predicted datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0bad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train AutoARIMA model\n",
    "model = auto_arima(train_data, trace=True, error_action='ignore', suppress_warnings=True)\n",
    "# Fit the model\n",
    "model.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba0e769",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [x for x in train_data]\n",
    "predicted_values_arima = []\n",
    "N_test_observations = len(test_data)\n",
    "for time_point in range(N_test_observations):\n",
    "    model = ARIMA(history, order=(2,1,1))\n",
    "    model_fit = model.fit()\n",
    "    output = model_fit.forecast()\n",
    "    yhat = output[0]\n",
    "    predicted_values_arima.append(yhat)\n",
    "    true_test_value = test_data[time_point]\n",
    "    history.append(true_test_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de863411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot the test data with a solid green line\n",
    "plt.plot(test_dates[60:], test_data[60:], color='green', label='Test Data', linestyle='-')\n",
    "\n",
    "# Plot the predicted values from the ARIMA model with a dashed red line\n",
    "plt.plot(test_dates[60:], predicted_values_arima[60:], color='red', linestyle='-', label='Predicted VIX Index (ARIMA model)')\n",
    "\n",
    "# Plot the predicted values from the MLP model with a dash-dot blue line\n",
    "plt.plot(test_dates[60:], predicted_values_mlp, color='blue', linestyle='--', label='Benchmark')\n",
    "\n",
    "plt.title('Test Data and Predicted Values')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('VIX index')\n",
    "\n",
    "# Move the legend outside the plot area to avoid overlapping the lines\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb8f5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(test_data, predicted_values_arima)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4f5c02",
   "metadata": {},
   "source": [
    "### Build_the_GARCH_model_using_HV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6240961",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split train and test data using sp500 return data\n",
    "sp500_data = sp500_returns.dropna().resample(\"W\").last()\n",
    "# Split the data into training, validation and test sets\n",
    "train2_size = int(len(sp500_data)*0.8)\n",
    "test2_size = len(sp500_data) - train2_size\n",
    "\n",
    "train2_data = sp500_data[:train2_size]\n",
    "test2_data = sp500_data[train2_size:]\n",
    "train2_dates = train2_data.index\n",
    "test2_dates = test2_data.index\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(train2_data, label=\"Training data\", color=\"blue\")\n",
    "plt.plot(test2_data, label=\"Testing data\", color=\"orange\")\n",
    "plt.title(\"S&P500 return data split\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a6d1d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the range of p and q values to consider\n",
    "p_values = range(1, 3)  # Change the range as per your preference\n",
    "q_values = range(1, 3)  # Change the range as per your preference\n",
    "\n",
    "# Initialize lists to store AIC and BIC values\n",
    "aic_values = []\n",
    "bic_values = []\n",
    "model_indices = []\n",
    "\n",
    "# Iterate over different p and q values\n",
    "for p in p_values:\n",
    "    for q in q_values:\n",
    "        model = arch.arch_model(sp500_data, vol='Garch', p=p, q=q)\n",
    "        model_fit = model.fit(disp='off')\n",
    "        aic_values.append(model_fit.aic)\n",
    "        bic_values.append(model_fit.bic)\n",
    "        model_indices.append((p, q))\n",
    "\n",
    "# Plot AIC values\n",
    "plt.plot(range(len(aic_values)), aic_values, marker='o')\n",
    "plt.xlabel('Model Index')\n",
    "plt.ylabel('AIC Value')\n",
    "plt.title('AIC Values for GARCH Models')\n",
    "plt.xticks(range(len(aic_values)), model_indices)\n",
    "plt.show()\n",
    "\n",
    "# Plot BIC values\n",
    "plt.plot(range(len(bic_values)), bic_values, marker='o')\n",
    "plt.xlabel('Model Index')\n",
    "plt.ylabel('BIC Value')\n",
    "plt.title('BIC Values for GARCH Models')\n",
    "plt.xticks(range(len(bic_values)), model_indices)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cc47c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [x for x in train2_data]\n",
    "predicted_values_garch = []\n",
    "N_test_observations = len(test2_data)\n",
    "for time_point in range(N_test_observations):\n",
    "    model = arch.arch_model(history, vol='Garch', p=1, q=1)\n",
    "    model_fit = model.fit()\n",
    "    output = model_fit.forecast().variance.values[-1]\n",
    "    yhat = np.sqrt(output[0])\n",
    "    predicted_values_garch.append(yhat)\n",
    "    true_test_value = test2_data[time_point]\n",
    "    history.append(true_test_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf2d5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the test data and predicted volatility\n",
    "plt.plot(test2_data.index, test2_data, color='blue', label='Test Data')\n",
    "plt.plot(test2_data.index, predicted_values_garch, color='red', label='Predicted Volatility')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Volatility')\n",
    "plt.title('Comparison of Test Data and Predicted Volatility')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8a4999",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(test2_data, predicted_values_garch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820a2374",
   "metadata": {},
   "source": [
    "ARIMA model performance is better than GARCH model. The following hybrid models will use ARIMA model as the econometric model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a48b87",
   "metadata": {},
   "source": [
    "### Build_ANN_model_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfb7d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparemeter tuning: grid search for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec5ea4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "train_data_normalized = scaler.fit_transform(train_data.values.reshape(-1, 1))\n",
    "test_data_normalized = scaler.transform(test_data.values.reshape(-1, 1))\n",
    "\n",
    "# Prepare the training data\n",
    "lookback = 60  # Number of previous time steps to consider\n",
    "\n",
    "def create_sequences(data, lookback):\n",
    "    X, y = [], []\n",
    "    for i in range(lookback, len(data)):\n",
    "        X.append(data[i - lookback:i, 0])\n",
    "        y.append(data[i, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = create_sequences(train_data_normalized, lookback)\n",
    "X_test, y_test = create_sequences(test_data_normalized, lookback)\n",
    "\n",
    "# Reshape the input data for LSTM (batch_size, timesteps, features)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Define the LSTM model\n",
    "def create_lstm_model(units=50, learning_rate=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=units, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(LSTM(units=units))\n",
    "    model.add(Dense(units=1))\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Wrap the Keras model as a scikit-learn estimator\n",
    "regressor = KerasRegressor(build_fn=create_lstm_model)\n",
    "\n",
    "# Perform hyperparameter tuning using grid search\n",
    "param_grid = {'units': [50, 100], 'batch_size': [32, 64], 'epochs': [10, 20], 'learning_rate': [0.001, 0.01, 0.1]}\n",
    "model = GridSearchCV(estimator=regressor, param_grid=param_grid, cv=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_units = model.best_params_['units']\n",
    "best_batch_size = model.best_params_['batch_size']\n",
    "best_epochs = model.best_params_['epochs']\n",
    "best_learning_rate = model.best_params_['learning_rate']\n",
    "\n",
    "print(f\"Best Hyperparameters: units={best_units}, batch_size={best_batch_size}, epochs={best_epochs}, learning_rate={best_learning_rate}\")\n",
    "\n",
    "# Train the LSTM model with the best hyperparameters\n",
    "final_model = create_lstm_model(units=best_units, learning_rate=best_learning_rate)\n",
    "final_model.fit(X_train, y_train, epochs=best_epochs, batch_size=best_batch_size)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predicted_values_lstm = final_model.predict(X_test)\n",
    "predicted_values_lstm = scaler.inverse_transform(predicted_values_lstm)\n",
    "\n",
    "# Plot the graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_dates[lookback:], test_data[lookback:], color='green', label='Test Data')\n",
    "plt.plot(test_dates[lookback:], predicted_values_lstm, color='red', linestyle='dashed', label='Predicted VIX Index (LSTM)')\n",
    "plt.title('Test Data and Predicted Values (LSTM)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('VIX index')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d11a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original version\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "train_data_normalized = scaler.fit_transform(train_data.values.reshape(-1, 1))\n",
    "\n",
    "# Prepare the training data\n",
    "lookback = 60  # Number of previous time steps to consider\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(lookback, len(train_data_normalized)):\n",
    "    X_train.append(train_data_normalized[i - lookback:i, 0])\n",
    "    y_train.append(train_data_normalized[i, 0])\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "# Reshape the input data for LSTM (batch_size, timesteps, features)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "# Build the LSTM model\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model_lstm.add(LSTM(units=50))\n",
    "model_lstm.add(Dense(units=1))\n",
    "model_lstm.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the LSTM model\n",
    "model_lstm.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Prepare the test data\n",
    "test_data_normalized = scaler.transform(test_data.values.reshape(-1, 1))\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for i in range(lookback, len(test_data_normalized)):\n",
    "    X_test.append(test_data_normalized[i - lookback:i, 0])\n",
    "    y_test.append(test_data_normalized[i, 0])\n",
    "\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "# Reshape the input data for LSTM (batch_size, timesteps, features)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Make predictions on the test data\n",
    "predicted_values_lstm = model_lstm.predict(X_test)\n",
    "predicted_values_lstm = scaler.inverse_transform(predicted_values_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d401e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(test_data[lookback:], predicted_values_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef1ab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Hyperparameters: units=50, batch_size=64, epochs=20, learning_rate=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd836d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimised version\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "train_data_normalized = scaler.fit_transform(train_data.values.reshape(-1, 1))\n",
    "\n",
    "# Prepare the training data\n",
    "lookback = 60  # Number of previous time steps to consider\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(lookback, len(train_data_normalized)):\n",
    "    X_train.append(train_data_normalized[i - lookback:i, 0])\n",
    "    y_train.append(train_data_normalized[i, 0])\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "# Reshape the input data for LSTM (batch_size, timesteps, features)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "# Build the LSTM model\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model_lstm.add(LSTM(units=50))\n",
    "model_lstm.add(Dense(units=1))\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "model_lstm.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the LSTM model\n",
    "model_lstm.fit(X_train, y_train, epochs=20, batch_size=64)\n",
    "\n",
    "# Prepare the test data\n",
    "test_data_normalized = scaler.transform(test_data.values.reshape(-1, 1))\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for i in range(lookback, len(test_data_normalized)):\n",
    "    X_test.append(test_data_normalized[i - lookback:i, 0])\n",
    "    y_test.append(test_data_normalized[i, 0])\n",
    "\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "# Reshape the input data for LSTM (batch_size, timesteps, features)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Make predictions on the test data\n",
    "predicted_values_lstm = model_lstm.predict(X_test)\n",
    "predicted_values_lstm = scaler.inverse_transform(predicted_values_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093934d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_dates[lookback:], test_data[lookback:], color='green', label='Test Data')\n",
    "plt.plot(test_dates[lookback:], predicted_values_lstm, color='red', linestyle='-', label='Predicted VIX Index (LSTM)')\n",
    "plt.plot(test_dates[lookback:], predicted_values_mlp, color='blue', linestyle='--', label='Benchmark')\n",
    "plt.title('Test Data and Predicted Values (LSTM)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('VIX index')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59755af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(test_data[lookback:], predicted_values_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6a29b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(test_data[lookback:], predicted_values_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec44975",
   "metadata": {},
   "source": [
    "### Build_ANN_model_GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e419e3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original version\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "train_data_normalized = scaler.fit_transform(train_data.values.reshape(-1, 1))\n",
    "\n",
    "# Prepare the training data\n",
    "lookback = 60  # Number of previous time steps to consider\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(lookback, len(train_data_normalized)):\n",
    "    X_train.append(train_data_normalized[i - lookback:i, 0])\n",
    "    y_train.append(train_data_normalized[i, 0])\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "# Reshape the input data for GRU (batch_size, timesteps, features)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "# Build the GRU model\n",
    "model_gru = Sequential()\n",
    "model_gru.add(GRU(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model_gru.add(GRU(units=50))\n",
    "model_gru.add(Dense(units=1))\n",
    "model_gru.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the GRU model\n",
    "model_gru.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Prepare the test data\n",
    "test_data_normalized = scaler.transform(test_data.values.reshape(-1, 1))\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for i in range(lookback, len(test_data_normalized)):\n",
    "    X_test.append(test_data_normalized[i - lookback:i, 0])\n",
    "    y_test.append(test_data_normalized[i, 0])\n",
    "\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "# Reshape the input data for GRU (batch_size, timesteps, features)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Make predictions on the test data\n",
    "predicted_values_gru = model_gru.predict(X_test)\n",
    "predicted_values_gru = scaler.inverse_transform(predicted_values_gru)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec20b893",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(test_data[lookback:], predicted_values_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e381a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimised version\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "train_data_normalized = scaler.fit_transform(train_data.values.reshape(-1, 1))\n",
    "\n",
    "# Prepare the training data\n",
    "lookback = 60  # Number of previous time steps to consider\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(lookback, len(train_data_normalized)):\n",
    "    X_train.append(train_data_normalized[i - lookback:i, 0])\n",
    "    y_train.append(train_data_normalized[i, 0])\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "# Reshape the input data for GRU (batch_size, timesteps, features)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "# Build the GRU model\n",
    "model_gru = Sequential()\n",
    "model_gru.add(GRU(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model_gru.add(GRU(units=50))\n",
    "model_gru.add(Dense(units=1))\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "model_gru.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the GRU model\n",
    "model_gru.fit(X_train, y_train, epochs=20, batch_size=64)\n",
    "\n",
    "# Prepare the test data\n",
    "test_data_normalized = scaler.transform(test_data.values.reshape(-1, 1))\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for i in range(lookback, len(test_data_normalized)):\n",
    "    X_test.append(test_data_normalized[i - lookback:i, 0])\n",
    "    y_test.append(test_data_normalized[i, 0])\n",
    "\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "# Reshape the input data for GRU (batch_size, timesteps, features)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Make predictions on the test data\n",
    "predicted_values_gru = model_gru.predict(X_test)\n",
    "predicted_values_gru = scaler.inverse_transform(predicted_values_gru)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277bab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_dates[lookback:], test_data[lookback:], color='green', label='Test Data')\n",
    "plt.plot(test_dates[lookback:], predicted_values_gru, color='red', linestyle='-', label='Predicted VIX Index (GRU)')\n",
    "plt.plot(test_dates[lookback:], predicted_values_mlp, color='blue', linestyle='--', label='Benchmark')\n",
    "\n",
    "\n",
    "plt.title('Test Data and Predicted Values (GRU)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('VIX index')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5701134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(test_data[lookback:], predicted_values_gru)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b530405c",
   "metadata": {},
   "source": [
    "### Build_LSTM-ARIMA_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2777ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_predictions_series = pd.Series(predicted_values_arima, index=test_dates)\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "train_data_normalized = scaler.fit_transform(model1_predictions_series.values.reshape(-1, 1))\n",
    "\n",
    "# Prepare the training data\n",
    "lookback = 60  # Number of previous time steps to consider\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(lookback, len(train_data_normalized)):\n",
    "    X_train.append(train_data_normalized[i - lookback:i, 0])\n",
    "    y_train.append(train_data_normalized[i, 0])\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "# Reshape the input data for LSTM (batch_size, timesteps, features)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "# Build the LSTM model\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model_lstm.add(LSTM(units=50))\n",
    "model_lstm.add(Dense(units=1))\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "model_lstm.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the LSTM model\n",
    "model_lstm.fit(X_train, y_train, epochs=20, batch_size=64)\n",
    "\n",
    "# Prepare the test data\n",
    "test_data_normalized = scaler.transform(test_data.values.reshape(-1, 1))\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for i in range(lookback, len(test_data_normalized)):\n",
    "    X_test.append(test_data_normalized[i - lookback:i, 0])\n",
    "    y_test.append(test_data_normalized[i, 0])\n",
    "\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "# Reshape the input data for LSTM (batch_size, timesteps, features)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Make predictions on the test data\n",
    "predicted_values_lstm_arima = scaler.inverse_transform(model_lstm.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e49322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_dates[lookback:], test_data[lookback:], color='green', label='Test Data')\n",
    "plt.plot(test_dates[lookback:], predicted_values_lstm_arima, color='red', linestyle='-', label='Predicted VIX Index (LSTM-ARIMA)')\n",
    "plt.plot(test_dates[lookback:], predicted_values_mlp, color='blue', linestyle='--', label='Benchmark')\n",
    "\n",
    "\n",
    "plt.title('Test Data and Predicted Values (LSTM_ARIMA)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('VIX index')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf06a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(test_data[lookback:], predicted_values_lstm_arima)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a0af09",
   "metadata": {},
   "source": [
    "### Build_ARIMA-LSTM_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9cb4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train AutoARIMA model\n",
    "history = [x[0] for x in predicted_values_lstm]\n",
    "model = auto_arima(history, trace=True, error_action='ignore', suppress_warnings=True)\n",
    "# Fit the model\n",
    "model.fit(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6cc3cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#One_step ahead forecast\n",
    "predicted_values_arima_lstm = []\n",
    "N_test_observations = len(test_data)\n",
    "for time_point in range(N_test_observations):\n",
    "    model = ARIMA(history, order=(3,1,3))\n",
    "    model_fit = model.fit()\n",
    "    output = model_fit.forecast()\n",
    "    yhat = output[0]\n",
    "    predicted_values_arima_lstm.append(yhat)\n",
    "    true_test_value = test_data[time_point]\n",
    "    history.append(true_test_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2acf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_dates[60:], test_data[60:], color='green', label='Test Data')\n",
    "plt.plot(test_dates[60:], predicted_values_arima_lstm[60:], color='red', linestyle='-', label='Predicted VIX Index (ARIMA-LSTM)')\n",
    "plt.plot(test_dates[60:], predicted_values_mlp, color='blue', linestyle='--', label='Benchmark')\n",
    "\n",
    "\n",
    "plt.title('Test Data and Predicted Values (ARIMA-LSTM)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('VIX index')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14588ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(test_data, predicted_values_arima_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb89b9f",
   "metadata": {},
   "source": [
    "### Build_GRU-ARIMA_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9a47f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_predictions_series = pd.Series(predicted_values_arima, index=test_dates)\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "train_data_normalized = scaler.fit_transform(model1_predictions_series.values.reshape(-1, 1))\n",
    "\n",
    "# Prepare the training data\n",
    "lookback = 60  # Number of previous time steps to consider\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(lookback, len(train_data_normalized)):\n",
    "    X_train.append(train_data_normalized[i - lookback:i, 0])\n",
    "    y_train.append(train_data_normalized[i, 0])\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "# Reshape the input data for GRU (batch_size, timesteps, features)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "# Build the GRU model\n",
    "model_gru = Sequential()\n",
    "model_gru.add(GRU(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model_gru.add(GRU(units=50))\n",
    "model_gru.add(Dense(units=1))\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "model_gru.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the GRU model\n",
    "model_gru.fit(X_train, y_train, epochs=20, batch_size=64)\n",
    "\n",
    "# Prepare the test data\n",
    "test_data_normalized = scaler.transform(test_data.values.reshape(-1, 1))\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for i in range(lookback, len(test_data_normalized)):\n",
    "    X_test.append(test_data_normalized[i - lookback:i, 0])\n",
    "    y_test.append(test_data_normalized[i, 0])\n",
    "\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "# Reshape the input data for GRU (batch_size, timesteps, features)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Make predictions on the test data\n",
    "predicted_values_gru_arima = scaler.inverse_transform(model_gru.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c967f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_dates[lookback:], test_data[lookback:], color='green', label='Test Data')\n",
    "plt.plot(test_dates[lookback:], predicted_values_gru_arima, color='red', linestyle='-', label='Predicted VIX Index (GRU-ARIMA)')\n",
    "plt.plot(test_dates[lookback:], predicted_values_mlp, color='blue', linestyle='--', label='Benchmark')\n",
    "\n",
    "plt.title('Test Data and Predicted Values (GRU-ARIMA)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('VIX index')\n",
    "plt.legend()\n",
    "#plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961129c7",
   "metadata": {},
   "source": [
    "### Build_ARIMA-GRU_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20530edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train AutoARIMA model\n",
    "history = [x[0] for x in predicted_values_gru]\n",
    "model = auto_arima(history, trace=True, error_action='ignore', suppress_warnings=True)\n",
    "# Fit the model\n",
    "model.fit(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6351bbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One_step ahead forecast\n",
    "predicted_values_arima_gru = []\n",
    "N_test_observations = len(test_data)\n",
    "for time_point in range(N_test_observations):\n",
    "    model = ARIMA(history, order=(2,1,1))\n",
    "    model_fit = model.fit()\n",
    "    output = model_fit.forecast()\n",
    "    yhat = output[0]\n",
    "    predicted_values_arima_gru.append(yhat)\n",
    "    true_test_value = test_data[time_point]\n",
    "    history.append(true_test_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a253511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_dates[60:], test_data[60:], color='green', label='Test Data')\n",
    "plt.plot(test_dates[60:], predicted_values_arima_gru[60:], color='red', linestyle='-', label='Predicted VIX Index (ARIMA-GRU)')\n",
    "plt.plot(test_dates[lookback:], predicted_values_mlp, color='blue', linestyle='--', label='Benchmark')\n",
    "\n",
    "plt.title('Test Data and Predicted Values (GARCH-GRU)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('VIX index')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9eff82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluation(test_data, predicted_values_arima_gru)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f87a41",
   "metadata": {},
   "source": [
    "## Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bdbccb",
   "metadata": {},
   "source": [
    "### Combine_models_prediction_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37b8ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lstm = pd.Series([x[0] for x in predicted_values_lstm], index = test_dates[60:])\n",
    "y_lstm_arima = pd.Series([x[0] for x in predicted_values_lstm_arima], index = test_dates[60:])\n",
    "y_arima_lstm = pd.Series(predicted_values_arima_lstm[60:],index =test_dates[60:])\n",
    "y_gru = pd.Series([x[0] for x in predicted_values_gru], index = test_dates[60:])\n",
    "y_gru_arima = pd.Series([x[0] for x in predicted_values_gru_arima], index = test_dates[60:])\n",
    "y_arima_gru = pd.Series(predicted_values_arima_gru[60:],index =test_dates[60:])\n",
    "y_benchmark = pd.Series([x[0] for x in predicted_values_mlp], index = test_dates[60:])\n",
    "y_test = test_data[60:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9646881",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "# Plot the y-values\n",
    "plt.plot(y_lstm, label='LSTM')\n",
    "plt.plot(y_lstm_arima, label='LSTM-ARIMA')\n",
    "plt.plot(y_arima_lstm, label='ARIMA-LSTM')\n",
    "plt.plot(y_gru, label='GRU')\n",
    "plt.plot(y_gru_arima, label='GRU-ARIMA')\n",
    "plt.plot(y_arima_gru, label='ARIMA-GRU')\n",
    "plt.plot(y_benchmark, label='Benchmark')\n",
    "plt.plot(y_test, label='Test Data')\n",
    "\n",
    "# Set the title and labels\n",
    "plt.title('Comparison of Predicted Values')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Volatility')\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f15a1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"Benchmark\",'LSTM', 'LSTM_ARIMA', 'ARIMA_LSTM', 'GRU', 'GRU_ARIMA', 'ARIMA_GRU',\"Test_data\"]\n",
    "predictions = [y_benchmark, y_lstm, y_lstm_arima, y_arima_lstm, y_gru, y_gru_arima, y_arima_gru,y_test]\n",
    "\n",
    "data_vol = {model: prediction for model, prediction in zip(models, predictions)}\n",
    "\n",
    "# Create the DataFrame\n",
    "df_vol = pd.DataFrame(data_vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447c263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb668ec",
   "metadata": {},
   "source": [
    "### Evaluation_method_1_index_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb26fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation method 1: index metrics (Prediction accuracy) Ref: Main1\n",
    "#1 Testing accuracy ratio for modelling (bar plot/box plot)\n",
    "#2 MSE, RMSE, MAE, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a36ebe0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate statistical measures for each model\n",
    "metrics = ['MSE', 'RMSE', 'MAE', 'Correlation', 'Cosine Similarity']\n",
    "results = []\n",
    "\n",
    "for column in df_vol.columns[:-1]:  # Exclude the 'Test_data' column\n",
    "    model_data = df_vol[column]\n",
    "    # Calculate statistical measures\n",
    "    mse = np.mean((model_data - df_vol['Test_data']) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(model_data - df_vol['Test_data']))\n",
    "    correlation = np.corrcoef(model_data, df_vol['Test_data'])[0, 1]\n",
    "    cosine_similarity = np.dot(model_data, df_vol['Test_data']) / (np.linalg.norm(model_data) * np.linalg.norm(df_vol['Test_data']))\n",
    "    \n",
    "    # Store the results\n",
    "    results.append([mse, rmse, mae, correlation, cosine_similarity])\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "df_results = pd.DataFrame(results, columns=metrics, index=df_vol.columns[:-1])\n",
    "\n",
    "# Sort the models based on the similarity metrics (choose the desired metric as per your preference)\n",
    "df_results_sorted_vol = df_results.sort_values(by='MSE', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ad4b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_sorted_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285d6a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c44a6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate relative MSE for each model\n",
    "models = df_vol.columns[:-1]  # Extract the model names from df_vol\n",
    "relative_mse_values = []\n",
    "\n",
    "for model in models:\n",
    "    mse_values = (df_vol['Test_data'] - df_vol[model]) ** 2\n",
    "    relative_mse = mse_values / np.mean(mse_values)\n",
    "    relative_mse_values.append(relative_mse)\n",
    "\n",
    "# Create boxplots of relative MSE for different models\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(relative_mse_values, vert=True, showfliers=False, notch=True, labels=models)\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Relative MSE')\n",
    "plt.title('Boxplot of Cross-Sectional Relative MSE for Different Models')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba8250d",
   "metadata": {},
   "source": [
    "### Evaluation_method_2_trading_strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db0721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input s&p500 historical data and create benchmark\n",
    "sp500 = pd.read_csv(\"SPY.csv\")\n",
    "sp500.index = pd.to_datetime(sp500[\"Date\"])\n",
    "sp500_return= sp500.loc[y_test.index[0]:y_test.index[-1]][\"Adj Close\"].pct_change()\n",
    "sp500_value = []\n",
    "value = 1\n",
    "for i in sp500_return[1:]:\n",
    "    value *= (1+i)\n",
    "    sp500_value.append(value)\n",
    "sp500_value.insert(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cead14",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"Benchmark\",'LSTM', 'LSTM_ARIMA', 'ARIMA_LSTM', 'GRU', 'GRU_ARIMA', 'ARIMA_GRU']\n",
    "predictions = [y_benchmark, y_lstm, y_lstm_arima, y_arima_lstm, y_gru, y_gru_arima, y_arima_gru]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a4e04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize portfolio values\n",
    "portfolio_values = pd.DataFrame(index=y_test.index, columns=models)\n",
    "y_test_return = y_test.pct_change()\n",
    "# Calculate portfolio values for each model\n",
    "for i, model in enumerate(models):\n",
    "    signal_models = np.sign(predictions[i].diff())  # Calculate signal based on predicted values\n",
    "    signal_test = np.sign(y_test.diff())\n",
    "    signal = [-1 if signal_models[i] == signal_test[i] else 1 for i in range(len(signal_models))]\n",
    "    portfolio_values[model] = (1 + signal * abs(y_test_return.shift(-3))).cumprod()  # Calculate portfolio values\n",
    "\n",
    "portfolio_values[\"S&P500\"] = sp500_value\n",
    "# Plot portfolio values\n",
    "plt.figure(figsize=(12, 6))\n",
    "for model in models:\n",
    "    plt.plot(portfolio_values.index, portfolio_values[model], label=model)\n",
    "plt.plot(portfolio_values.index, sp500_value, label=\"S&P 500\")\n",
    "plt.title(\"Portfolio Values\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bf201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate daily returns for each model\n",
    "daily_returns = portfolio_values.pct_change()\n",
    "\n",
    "# Calculate Sharpe ratio\n",
    "risk_free_rate = 0.0  # Set the risk-free rate\n",
    "sharpe_ratio = (daily_returns.mean() - risk_free_rate) / daily_returns.std()\n",
    "\n",
    "# Create table for daily returns\n",
    "daily_returns_table = pd.DataFrame(daily_returns.mean(), columns=[\"Weekly Return\"])\n",
    "daily_returns_table[\"Std Dev\"] = daily_returns.std()\n",
    "daily_returns_table[\"Sharpe Ratio\"] = sharpe_ratio\n",
    "daily_returns_table = daily_returns_table.round(4)\n",
    "\n",
    "# Sort the table by \"Weekly Return\" column in descending order\n",
    "daily_returns_table = daily_returns_table.sort_values(by=\"Weekly Return\", ascending=False)\n",
    "\n",
    "# Display table for daily returns and Sharpe ratio\n",
    "print(\"Weekly Returns and Sharpe Ratio:\")\n",
    "daily_returns_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9de040f",
   "metadata": {},
   "source": [
    "### Evaluation_method_3_Option_pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c2c43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Black-Scholes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7845c096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_scholes(S, K, r, T, sigma, option_type):\n",
    "    d1 = (math.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * math.sqrt(T))\n",
    "    d2 = d1 - sigma * math.sqrt(T)\n",
    "    \n",
    "    if option_type == 'call':\n",
    "        price = S * norm.cdf(d1) - K * math.exp(-r * T) * norm.cdf(d2)\n",
    "    elif option_type == 'put':\n",
    "        price = K * math.exp(-r * T) * norm.cdf(-d2) - S * norm.cdf(-d1)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid option type. Choose 'call' or 'put'.\")\n",
    "    \n",
    "    return price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c2925b",
   "metadata": {},
   "source": [
    "#### Call_option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7a6c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Time series data for S&P 500 stock prices\n",
    "sp500_price = sp500.loc[y_test.index[0]:y_test.index[-1]][\"Adj Close\"]\n",
    "\n",
    "# Volatility predictions for each model\n",
    "models = [\"Benchmark\", 'LSTM', 'LSTM_ARIMA', 'ARIMA_LSTM', 'GRU', 'GRU_ARIMA', 'ARIMA_GRU',\"Test data\"]\n",
    "predictions = [y_benchmark, y_lstm, y_lstm_arima, y_arima_lstm, y_gru, y_gru_arima, y_arima_gru,y_test]\n",
    "\n",
    "# Parameters for option pricing\n",
    "risk_free_rate = 0.05  # Risk-free interest rate (r)\n",
    "time_to_maturity = 0.5  # Time to option expiration in years (T)\n",
    "\n",
    "# Generate option prices for each model\n",
    "option_prices = []\n",
    "for model, volatility in zip(models, predictions):\n",
    "    option_prices_model = []\n",
    "    for stock_price, sigma in zip(sp500_price, volatility):\n",
    "        strike_price = stock_price + 50  # Strike price is $50 higher than S&P 500 price\n",
    "        option_price = black_scholes(stock_price, strike_price, risk_free_rate, time_to_maturity, 0.01*sigma, 'call')\n",
    "        option_prices_model.append(option_price)\n",
    "    option_prices.append(option_prices_model)\n",
    "\n",
    "# Create a DataFrame from the option prices\n",
    "data_option = list(map(list, zip(*option_prices)))\n",
    "df_option = pd.DataFrame(data_option, columns=models)\n",
    "df_option = df_option.set_index(y_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abc3f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the time series data for each model\n",
    "plt.figure(figsize=(12, 6))\n",
    "for column in df_option.columns:\n",
    "    plt.plot(df_option.index, df_option[column], label=column)\n",
    "\n",
    "# Set the title, x-axis label, and y-axis label\n",
    "plt.title(\"Comparison of different models' predicted call option prices\")\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Call option price')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e71d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate statistical measures for each model\n",
    "metrics = ['MSE', 'RMSE', 'MAE', 'Correlation', 'Cosine Similarity']\n",
    "results = []\n",
    "\n",
    "for column in df_option.columns[:-1]:  # Exclude the 'Test_data' column\n",
    "    model_data = df_option[column]\n",
    "    \n",
    "    # Calculate statistical measures\n",
    "    mse = np.mean((model_data - df_option['Test data']) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(model_data - df_option['Test data']))\n",
    "    correlation = np.corrcoef(model_data, df_option['Test data'])[0, 1]\n",
    "    cosine_similarity = np.dot(model_data, df_option['Test data']) / (np.linalg.norm(model_data) * np.linalg.norm(df_option['Test data']))\n",
    "    \n",
    "    # Store the results\n",
    "    results.append([mse, rmse, mae, correlation, cosine_similarity])\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "df_results = pd.DataFrame(results, columns=metrics, index=df_option.columns[:-1])\n",
    "\n",
    "# Sort the models based on the similarity metrics (choose the desired metric as per your preference)\n",
    "df_results_sorted_option = df_results.sort_values(by='MSE', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b77b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_sorted_option"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6abb2c5",
   "metadata": {},
   "source": [
    "#### Put_option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e96532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Time series data for S&P 500 stock prices\n",
    "sp500_price = sp500.loc[y_test.index[0]:y_test.index[-1]][\"Adj Close\"]\n",
    "\n",
    "# Volatility predictions for each model\n",
    "models = [\"Benchmark\", 'LSTM', 'LSTM_ARIMA', 'ARIMA_LSTM', 'GRU', 'GRU_ARIMA', 'ARIMA_GRU',\"Test data\"]\n",
    "predictions = [y_benchmark, y_lstm, y_lstm_arima, y_arima_lstm, y_gru, y_gru_arima, y_arima_gru,y_test]\n",
    "\n",
    "# Parameters for option pricing\n",
    "risk_free_rate = 0.05  # Risk-free interest rate (r)\n",
    "time_to_maturity = 0.5  # Time to option expiration in years (T)\n",
    "\n",
    "# Generate option prices for each model\n",
    "option_prices = []\n",
    "for model, volatility in zip(models, predictions):\n",
    "    option_prices_model = []\n",
    "    for stock_price, sigma in zip(sp500_price, volatility):\n",
    "        strike_price = stock_price - 50  # Strike price is $50 higher than S&P 500 price\n",
    "        option_price = black_scholes(stock_price, strike_price, risk_free_rate, time_to_maturity, 0.01*sigma, 'put')\n",
    "        option_prices_model.append(option_price)\n",
    "    option_prices.append(option_prices_model)\n",
    "\n",
    "# Create a DataFrame from the option prices\n",
    "data_option = list(map(list, zip(*option_prices)))\n",
    "df_option = pd.DataFrame(data_option, columns=models)\n",
    "df_option = df_option.set_index(y_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2582a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the time series data for each model\n",
    "plt.figure(figsize=(12, 6))\n",
    "for column in df_option.columns:\n",
    "    plt.plot(df_option.index, df_option[column], label=column)\n",
    "\n",
    "# Set the title, x-axis label, and y-axis label\n",
    "plt.title(\"Comparison of different models' predicted put option prices\")\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Put option price')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f038c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate statistical measures for each model\n",
    "metrics = ['MSE', 'RMSE', 'MAE', 'Correlation', 'Cosine Similarity']\n",
    "results = []\n",
    "\n",
    "for column in df_option.columns[:-1]:  # Exclude the 'Test_data' column\n",
    "    model_data = df_option[column]\n",
    "    \n",
    "    # Calculate statistical measures\n",
    "    mse = np.mean((model_data - df_option['Test data']) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(model_data - df_option['Test data']))\n",
    "    correlation = np.corrcoef(model_data, df_option['Test data'])[0, 1]\n",
    "    cosine_similarity = np.dot(model_data, df_option['Test data']) / (np.linalg.norm(model_data) * np.linalg.norm(df_option['Test data']))\n",
    "    \n",
    "    # Store the results\n",
    "    results.append([mse, rmse, mae, correlation, cosine_similarity])\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "df_results = pd.DataFrame(results, columns=metrics, index=df_option.columns[:-1])\n",
    "\n",
    "# Sort the models based on the similarity metrics (choose the desired metric as per your preference)\n",
    "df_results_sorted_option = df_results.sort_values(by='MSE', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28622cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_sorted_option"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "623.993px",
    "width": "459.063px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
